# Core API config
OPENROUTER_API_KEY=                 # OpenRouter API key
OPENROUTER_HTTP_REFERER=http://localhost    # Optional: OpenRouter analytics referrer
OPENROUTER_APP_TITLE=HalluLens              # Optional: OpenRouter app title
OPENROUTER_MODEL=                           # Optional: default model override

OPENROUTER_MAX_RETRIES=100
OPENROUTER_RETRY_BASE_SECONDS=1
OPENROUTER_RETRY_MAX_SECONDS=60
OPENROUTER_EMPTY_RETRIES=10
# Optional LM Studio config (set USE_LM_STUDIO=true to enable)
USE_LM_STUDIO=false                         # true=use LM Studio instead of OpenRouter
LM_STUDIO_URL=http://10.10.12.21:1234/v1/chat/completions  # LM Studio endpoint
LM_STUDIO_MODEL=openai/gpt-oss-20b          # LM Studio model name




# Global experiment config
EXP_MODE=longwiki                            # longwiki or hybrid
N=300                                          # number of prompts to generate
DB_PATH=data/wiki_data/.cache/enwiki-20230401.db  # SQLite DB for retrieval/eval

# ===== Prompt generation (LongWiki / Hybrid) =====
MODEL_PROMPT=xiaomi/mimo-v2-flash    # model used to generate prompts
TASKS=INTERVIEW NEWS_ARTICLE                 # task list (space-separated)
CREATIVITY=FACTUAL HYBRID VERY_CREATIVE      # creativity levels (space-separated)
LENGTH_WORDS=50                              # target length for prompts
LENGTH_WORDS_LIST=50 150 300                 # optional: grid of prompt lengths
LOW_LEVEL=5                                  # min h_score_cat
HIGH_LEVEL=10                                # max h_score_cat (exclusive)
STATIC_USER_PROMPT=true                     # true=deterministic prompt template
PROMPT_SEED=42                               # seed for deterministic subject selection

# ===== Inference (model under test) =====
MODEL_RESPONSE=mistralai/mistral-small-creative  # model being evaluated
INFER_MODELS=mistralai/mistral-small-creative    # optional: grid of inference models
INFERENCE_METHOD=custom                          # custom/openai/vllm
TEMPERATURE=0.0                                  # decoding temperature
INFER_TEMPS=0.0 0.7 1.0                           # optional: grid of temperatures
MAX_TOKENS=1024                                  # max tokens per response
OPENROUTER_MAX_IN_FLIGHT=128
MAX_WORKERS=96
PROMPT_MAX_WORKERS=64
EVAL_MAX_WORKERS=64                               # parallel requests
RETRIEVAL_BATCH_SIZE=64                           # sentence-transformers batch size
RETRIEVAL_QUERY_BATCH_SIZE=64                     # batch size for query embeddings (Step 3)
RETRIEVAL_PAGE_CACHE_SIZE=1024                    # in-memory title->passages cache (0 disables)
NER_BATCH_SIZE=32                                 # NER pipeline batch size
VERIFY_PROMPT_MAX_WORKERS=8                       # parallel prompt prep in Step 3
VERIFY_PROMPT_PRECOMPUTE_QUERIES=true             # precompute query embeddings for prompt prep

# ===== Evaluation (hallucination scoring) =====
MODEL_EVAL=xiaomi/mimo-v2-flash          # default eval model
ABSTAIN_EVALUATOR=xiaomi/mimo-v2-flash   # refusal/abstain checker
CLAIM_EXTRACTOR=xiaomi/mimo-v2-flash     # claim extraction model
VERIFIER=xiaomi/mimo-v2-flash            # claim verification model
K=32                                             # top-k evidence for recall
EVAL_CACHE_PATH=                                 # optional cache dir
CACHE_NAMESPACE=                                 # auto | none | <name>

# ===== Experiment grid runner =====
RUN_NAMESPACE_BASE=static_prompts                # base name for prompt/inference runs
PROMPT_MODEL_TAG=promptset                       # tag used for prompt file naming
FORCE_REGEN=false                                # true=regenerate prompts even if cached
SKIP_EVAL=false                                  # true=skip evaluation step


