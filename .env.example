# Core API config
OPENROUTER_API_KEY=                 # OpenRouter API key
OPENROUTER_HTTP_REFERER=http://localhost    # Optional: OpenRouter analytics referrer
OPENROUTER_APP_TITLE=HalluLens              # Optional: OpenRouter app title
OPENROUTER_MODEL=                           # Optional: default model override

OPENROUTER_MAX_RETRIES=100
OPENROUTER_RETRY_BASE_SECONDS=1
OPENROUTER_RETRY_MAX_SECONDS=60
OPENROUTER_EMPTY_RETRIES=10
# Optional LM Studio config (set USE_LM_STUDIO=true to enable)
USE_LM_STUDIO=false                         # true=use LM Studio instead of OpenRouter
LM_STUDIO_URL=http://10.10.12.21:1234/v1/chat/completions  # LM Studio endpoint
LM_STUDIO_MODEL=openai/gpt-oss-20b          # LM Studio model name




# Global experiment config
EXP_MODE=longwiki                            # longwiki or hybrid
N=300                                          # number of prompts to generate
DB_PATH=data/wiki_data/.cache/enwiki-20230401.db  # SQLite DB for retrieval/eval

# ===== Prompt generation (LongWiki / Hybrid) =====
MODEL_PROMPT=xiaomi/mimo-v2-flash    # model used to generate prompts
TASKS=INTERVIEW NEWS_ARTICLE                 # task list (space-separated)
CREATIVITY=FACTUAL HYBRID VERY_CREATIVE      # creativity levels (space-separated)
LENGTH_WORDS=50                              # target length for prompts
LENGTH_WORDS_LIST=50 150                 # optional: grid of prompt lengths
LOW_LEVEL=5                                  # min h_score_cat
HIGH_LEVEL=10                                # max h_score_cat (exclusive)
STATIC_USER_PROMPT=true                     # true=deterministic prompt template
PROMPT_SEED=42                               # seed for deterministic subject selection

# ===== Inference (model under test) =====
MODEL_RESPONSE=mistralai/mistral-small-creative  # model being evaluated
INFER_MODELS=mistralai/mistral-small-creative    # optional: grid of inference models
INFERENCE_METHOD=custom                          # custom/openai/vllm
TEMPERATURE=0.0                                  # decoding temperature
INFER_TEMPS=0.0 0.25 0.5 0.75                           # optional: grid of temperatures
MAX_TOKENS=1024                                  # max tokens per response
OPENROUTER_MAX_IN_FLIGHT=128
MAX_WORKERS=96
PROMPT_MAX_WORKERS=64
EVAL_MAX_WORKERS=64                               # parallel requests
RETRIEVAL_BATCH_SIZE=512                           # sentence-transformers batch size
RETRIEVAL_QUERY_BATCH_SIZE=512                     # batch size for query embeddings (Step 3)
RETRIEVAL_PAGE_CACHE_SIZE=1024
RETRIEVAL_DB_BATCH_TITLES=512                     # batched DB fetch size (<= 900, higher reduces overhead)
RETRIEVAL_CACHE_WRITE=true                        # write per-claim top-k passage cache (can be slow)
EMBED_CACHE_WRITE=true                            # persist passage-embedding cache to disk
EMBED_CACHE_SAVE_EVERY=500                         # flush embedding cache every N new titles
CACHE_COMMIT_EVERY=256                             # sqlite cache commit batching (1 = safest, slower)                    # in-memory title->passages cache (0 disables)
RETRIEVAL_USE_NER=true                            # disable NER to speed up (uses only topic)
RETRIEVAL_USE_RELEVANT_TITLES=true                # disable title LIKE search to speed up
RETRIEVAL_MAX_NER=0                               # cap NER entities per question (0 = unlimited)
RETRIEVAL_MIN_NER_LEN=0                           # drop very short entities (0 = keep all)
RETRIEVAL_MAX_TITLES_PER_NER=0                    # cap titles per NER (0 = unlimited)
RETRIEVAL_TITLE_MATCH_MODE=contains               # contains | prefix | exact
NER_BATCH_SIZE=32                                 # NER pipeline batch size
VERIFY_PROMPT_MAX_WORKERS=8                       # parallel prompt prep in Step 3
VERIFY_PROMPT_PRECOMPUTE_QUERIES=true             # precompute query embeddings for prompt prep
VERIFY_PROMPT_PROFILE=false                       # print Step 3 prompt prep timing breakdown

# ===== Profiles (copy one block into the section above) =====
# -- SAFE (max fidelity, slower) --
# RETRIEVAL_BATCH_SIZE=64
# RETRIEVAL_QUERY_BATCH_SIZE=64
# RETRIEVAL_DB_BATCH_TITLES=128
# RETRIEVAL_PAGE_CACHE_SIZE=1024
# RETRIEVAL_USE_NER=true
# RETRIEVAL_USE_RELEVANT_TITLES=true
# RETRIEVAL_MAX_NER=0
# RETRIEVAL_MAX_TITLES_PER_NER=0
# RETRIEVAL_TITLE_MATCH_MODE=contains
# RETRIEVAL_CACHE_WRITE=true
# EMBED_CACHE_WRITE=true
# EMBED_CACHE_SAVE_EVERY=50
# CACHE_COMMIT_EVERY=64
# NER_BATCH_SIZE=32
# VERIFY_PROMPT_MAX_WORKERS=8
# VERIFY_PROMPT_PRECOMPUTE_QUERIES=true

# -- FAST (balanced speed / some recall drop) --
RETRIEVAL_BATCH_SIZE=128
RETRIEVAL_QUERY_BATCH_SIZE=128
RETRIEVAL_DB_BATCH_TITLES=256
RETRIEVAL_PAGE_CACHE_SIZE=2048
RETRIEVAL_USE_NER=true
RETRIEVAL_USE_RELEVANT_TITLES=false
RETRIEVAL_MAX_NER=0
RETRIEVAL_MIN_NER_LEN=0
RETRIEVAL_MAX_TITLES_PER_NER=0
RETRIEVAL_TITLE_MATCH_MODE=prefix
RETRIEVAL_CACHE_WRITE=false
EMBED_CACHE_WRITE=false
EMBED_CACHE_SAVE_EVERY=200
CACHE_COMMIT_EVERY=128
NER_BATCH_SIZE=64
VERIFY_PROMPT_MAX_WORKERS=8
VERIFY_PROMPT_PRECOMPUTE_QUERIES=true

# -- FASTEST (aggressive, lowest recall) --
# RETRIEVAL_BATCH_SIZE=256
# RETRIEVAL_QUERY_BATCH_SIZE=256
# RETRIEVAL_DB_BATCH_TITLES=512
# RETRIEVAL_PAGE_CACHE_SIZE=4096
# RETRIEVAL_USE_NER=false
# RETRIEVAL_USE_RELEVANT_TITLES=false
# RETRIEVAL_MAX_NER=0
# RETRIEVAL_MIN_NER_LEN=0
# RETRIEVAL_MAX_TITLES_PER_NER=0
# RETRIEVAL_TITLE_MATCH_MODE=contains
# RETRIEVAL_CACHE_WRITE=false
# EMBED_CACHE_WRITE=false
# EMBED_CACHE_SAVE_EVERY=1000
# CACHE_COMMIT_EVERY=256
# NER_BATCH_SIZE=64
# VERIFY_PROMPT_MAX_WORKERS=8
# VERIFY_PROMPT_PRECOMPUTE_QUERIES=true

# ===== Evaluation (hallucination scoring) =====
MODEL_EVAL=xiaomi/mimo-v2-flash          # default eval model
ABSTAIN_EVALUATOR=xiaomi/mimo-v2-flash   # refusal/abstain checker
CLAIM_EXTRACTOR=xiaomi/mimo-v2-flash     # claim extraction model
VERIFIER=xiaomi/mimo-v2-flash            # claim verification model
K=32                                             # top-k evidence for recall
EVAL_CACHE_PATH=                                 # optional cache dir
CACHE_NAMESPACE=                                 # auto | none | <name>

# ===== Experiment grid runner =====
RUN_NAMESPACE_BASE=static_prompts                # base name for prompt/inference runs
PROMPT_MODEL_TAG=promptset                       # tag used for prompt file naming
FORCE_REGEN=false                                # true=regenerate prompts even if cached
SKIP_EVAL=false                                  # true=skip evaluation step
