# Patchs Appliqu√©s : Configuration Temp√©rature Mod√®le d'Inf√©rence

## üìã Format Obligatoire de R√©ponse

### 1. Objectif
Permettre la configuration de la **temp√©rature** et du **max_tokens** pour le mod√®le d'inf√©rence (g√©n√©ration de r√©ponses) via des arguments CLI, tout en gardant des temp√©ratures **fixes** pour les autres √©tapes (g√©n√©ration de prompts, √©valuation).

### 2. Fichiers Touch√©s
1. `tasks/longwiki/longwiki_main.py` - Ajout argument CLI et propagation
2. `utils/exp.py` - Acceptation param√®tre temperature
3. `utils/generate_question.py` - Correction bug answerability batch
4. `scripts/task2_longwiki_openrouter.sh` - Variables d'environnement
5. `scripts/task2_longwiki.sh` - Variables d'environnement

### 3. Strat√©gie de Test
- ‚úÖ Validation signature fonction `exp.run_exp()`
- ‚úÖ V√©rification arguments CLI `--temperature`
- ‚úÖ Test r√©trocompatibilit√© (appels sans temperature)
- ‚úÖ Validation temp√©ratures fixes (g√©n√©ration questions, √©valuation)
- ‚úÖ Test syntaxe Python (py_compile)

### 4. Patchs (diff)

#### Patch 1 : `tasks/longwiki/longwiki_main.py`

**Ajout argument CLI temperature :**
```diff
     parser.add_argument("--k", type=int, default=32)
     parser.add_argument("--max_tokens", type=int, default=1024)
+    parser.add_argument(
+        "--temperature",
+        type=float,
+        default=0.0,
+        help="Temperature for inference model (0.0 = deterministic, higher = more random)",
+    )
     parser.add_argument("--max_workers", type=int, default=64)
     args = parser.parse_args()
```

**Passage des param√®tres √† exp.run_exp() :**
```diff
         exp.run_exp(
             task=f"{TASKNAME}-{args.exp_mode}",
             model_path=args.model,
             all_prompts=all_prompts,
             inference_method=args.inference_method,
             max_tokens=args.max_tokens,
+            temperature=args.temperature,
+            max_workers=args.max_workers,
         )
```

#### Patch 2 : `utils/exp.py`

**Ajout param√®tre temperature avec valeur par d√©faut :**
```diff
 def run_exp(
     task: str,
     model_path: str,
     all_prompts,
     generations_file_path=None,
     base_path="output",
     inference_method="vllm",
     max_workers=64,
     max_tokens=512,
+    temperature=0.0,
     return_gen = False
 ):
```

**Utilisation du param√®tre au lieu de valeur cod√©e en dur :**
```diff
     # Always use OpenRouter for LLM generation
     all_prompts["generation"] = thread_map(
-        lambda p: lm.generate(p, model=model_path, temperature=0.0, top_p=1.0, max_tokens=max_tokens),
+        lambda p: lm.generate(p, model=model_path, temperature=temperature, top_p=1.0, max_tokens=max_tokens),
         prompts,
         max_workers=max_workers,
         desc="Predict on OpenRouter",
     )
```

#### Patch 3 : `utils/generate_question.py`

**Correction bug answerability check batch (ligne 220) :**
```diff
         print("Generating answers...")
-        ans_results = thread_map(lambda p: lm.generate(p, self.q_generator),
+        ans_results = thread_map(lambda p: lm.generate(p, self.q_generator, temperature=0.3),
                                     prompts_answerability,
                                     max_workers=50,
                                     desc=f"using {self.q_generator}")
```

**Raison** : Harmonisation avec l'answerability check non-batch (ligne 145) qui utilise `temperature=0.3`.

#### Patch 4 : `scripts/task2_longwiki_openrouter.sh`

**Ajout variables d'environnement configurables :**
```diff
 MODEL_RESPONSE="mistralai/mistral-small-creative"
 MODEL_PROMPT="deepseek/deepseek-v3.2"
 MODEL_EVAL="deepseek/deepseek-v3.2"
 
+# Inference parameters (can be overridden via environment variables)
+TEMPERATURE="${TEMPERATURE:-0.0}"
+MAX_TOKENS="${MAX_TOKENS:-1024}"
+MAX_WORKERS="${MAX_WORKERS:-64}"
+
 python -m tasks.longwiki.longwiki_main \
   --exp_mode "${EXP_MODE}" \
   --do_generate_prompt \
   --do_inference \
   --do_eval \
   --model "${MODEL_RESPONSE}" \
   --q_generator "${MODEL_PROMPT}" \
   --abstain_evaluator "${MODEL_EVAL}" \
   --claim_extractor "${MODEL_EVAL}" \
   --verifier "${MODEL_EVAL}" \
   --db_path "${DB_PATH}" \
-  --N "${N}"
+  --N "${N}" \
+  --temperature "${TEMPERATURE}" \
+  --max_tokens "${MAX_TOKENS}" \
+  --max_workers "${MAX_WORKERS}"
```

#### Patch 5 : `scripts/task2_longwiki.sh`

**Modifications identiques au script OpenRouter.**

### 5. Commandes de V√©rification

#### V√©rification 1 : Syntaxe Python valide
```bash
python -m py_compile tasks/longwiki/longwiki_main.py utils/exp.py utils/generate_question.py
# Doit se terminer sans erreur
```

#### V√©rification 2 : Argument CLI disponible
```bash
python -m tasks.longwiki.longwiki_main --help | grep -A 2 temperature
# Attendu:
#   --temperature TEMPERATURE
#                         Temperature for inference model (0.0 = deterministic,
#                         higher = more random)
```

#### V√©rification 3 : Temp√©ratures fixes pour g√©n√©ration questions
```bash
grep -n "temperature=0.7" utils/generate_question.py
# Attendu: lignes 136 et 206 (g√©n√©ration questions)

grep -n "temperature=0.3" utils/generate_question.py
# Attendu: lignes 145 et 220 (answerability check)
```

#### V√©rification 4 : Temp√©rature fixe pour √©valuation
```bash
grep -n "lm.generate" tasks/longwiki/longwiki_utils.py
# Attendu: ligne 128 sans param√®tre temperature (utilise d√©faut 0.0)
```

#### V√©rification 5 : Test signature fonction
```python
import inspect
from utils import exp

sig = inspect.signature(exp.run_exp)
params = sig.parameters

# V√©rifier que temperature est un param√®tre
assert 'temperature' in params
print("‚úÖ Parameter 'temperature' exists")

# V√©rifier valeur par d√©faut
assert params['temperature'].default == 0.0
print("‚úÖ Default temperature is 0.0")

# V√©rifier que temperature n'est pas requis (r√©trocompatibilit√©)
required_params = [p for p in params.keys() 
                   if params[p].default == inspect.Parameter.empty]
assert 'temperature' not in required_params
print("‚úÖ Backward compatibility maintained")
```

#### V√©rification 6 : Test complet d'ex√©cution (dry-run)
```bash
# Test avec temp√©rature par d√©faut
python -m tasks.longwiki.longwiki_main \
  --exp_mode longwiki \
  --model "test-model" \
  --N 1 \
  --help

# Test avec temp√©rature personnalis√©e
python -m tasks.longwiki.longwiki_main \
  --exp_mode longwiki \
  --model "test-model" \
  --temperature 0.7 \
  --max_tokens 2048 \
  --N 1 \
  --help
```

#### V√©rification 7 : Validation scripts shell
```bash
# V√©rifier syntaxe bash
bash -n scripts/task2_longwiki_openrouter.sh
bash -n scripts/task2_longwiki.sh

# Dry-run avec variables d'environnement
TEMPERATURE=0.5 MAX_TOKENS=512 bash -x scripts/task2_longwiki_openrouter.sh 2>&1 | grep temperature
# Devrait afficher: --temperature 0.5
```

### 6. Notes de Migration

#### ‚úÖ Aucune action requise pour les utilisateurs existants

**R√©trocompatibilit√© garantie** :
- Les appels existants √† `exp.run_exp()` sans param√®tre `temperature` continuent de fonctionner
- Le comportement par d√©faut reste identique (`temperature=0.0`)
- Aucune modification n√©cessaire dans :
  - `tasks/refusal_test/nonsense_name.py`
  - `tasks/refusal_test/round_robin_nonsense_name.py`
  - `tasks/shortform/precise_wikiqa.py`

#### üéØ Pour b√©n√©ficier de la nouvelle fonctionnalit√©

**Option 1 : Via CLI**
```bash
python -m tasks.longwiki.longwiki_main \
  --temperature 0.7 \
  --max_tokens 2048 \
  ...
```

**Option 2 : Via variables d'environnement**
```bash
TEMPERATURE=0.7 MAX_TOKENS=2048 bash scripts/task2_longwiki_openrouter.sh
```

**Option 3 : Via code Python**
```python
from utils import exp

exp.run_exp(
    task="longwiki",
    model_path="model-name",
    all_prompts=prompts,
    temperature=0.7,  # ‚Üê Nouveau param√®tre optionnel
    max_tokens=2048
)
```

#### ‚ö†Ô∏è Breaking Changes : AUCUN

Tous les appels existants restent compatibles sans modification.

---

## üß™ Tests de Non-R√©gression

### Test 1 : Anciens appels sans temperature
```python
# Ce code doit continuer √† fonctionner
exp.run_exp(
    task="test",
    model_path="model",
    all_prompts=df
    # temperature non sp√©cifi√© ‚Üí utilise 0.0
)
```

### Test 2 : Nouveaux appels avec temperature
```python
# Ce code doit maintenant fonctionner
exp.run_exp(
    task="test",
    model_path="model",
    all_prompts=df,
    temperature=0.7  # ‚Üê Nouveau
)
```

### Test 3 : Temp√©ratures fixes restent intactes
```python
# Ces appels NE DOIVENT PAS √™tre affect√©s
WikiQA.generate_question_with_doc(...)  # temperature=0.7 (fixe)
WikiQA.generate_answerability(...)      # temperature=0.3 (fixe)
model_eval_step(...)                    # temperature=0.0 (fixe)
```

---

## üìä R√©sum√© des Changements

| Aspect | Avant | Apr√®s |
|--------|-------|-------|
| Temp√©rature inf√©rence | 0.0 (cod√© en dur) | **CLI configurable** (d√©faut: 0.0) |
| Temp√©rature questions | 0.7 (fixe) | 0.7 (fixe) ‚úÖ |
| Temp√©rature answerability | 0.3 / **0.0 bug** | 0.3 (fixe) ‚úÖ corrig√© |
| Temp√©rature √©valuation | 0.0 (fixe) | 0.0 (fixe) ‚úÖ |
| R√©trocompatibilit√© | N/A | ‚úÖ Pr√©serv√©e |
| Breaking changes | N/A | ‚ùå Aucun |

**‚úÖ VALIDATION : Tous les objectifs sont atteints sans r√©gression.**
